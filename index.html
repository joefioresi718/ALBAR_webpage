<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ALBAR utilizes adversarial learning to mitigate foreground and background biases in video action recognition.">
  <meta name="keywords" content="action recognition, foreground bias, background bias, ALBAR, adversarial learning, ICLR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ucf_hotbar_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition [ICLR 2025]</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://joefioresi718.github.io/">Joseph Fioresi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://daveishan.github.io/">Ishan Rajendrakumar Dave</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en&oi=ao">Mubarak Shah</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Center for Research in Computer Vision (CRCV), University of Central Florida</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.11072"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/3a9qeJUD1GU?si=DlpYsj1AFW6iQX1J"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UCF-CRCV/ALBAR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Poster Link -->
              <span class="link-block">
                <a href="./static/images/albar_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
	<center>
		<div class="container is-max-desktop">
			<div class="hero-body">
			  <img src="./static/images/TeaserFigure.svg" alt="ALBAR Teaser" class="teaser-image">
			  <h2 class="subtitle has-text-centered">
				ALBAR classifies videos without relying on background or static foreground information.
			  </h2>
			</div>
		  </div>
	</center>
</section>

<style> 
    /* .results-carousel {
      width: 100%;
      height: 100vh;
      display: flex;
      flex-wrap: nowrap;
      scroll-snap-type: x mandatory;
      -webkit-overflow-scrolling: touch;
    } */

	.gif-pair {
      width: 100%;
      height: 100%;
      scroll-snap-align: start;
      display: flex;
      flex-wrap: wrap;
      justify-content: space-evenly;
      /* align-items: center;
	  align-self: center; */
      /* padding: 10px; */
    }

</style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases.
          </p>
          <p>
            In this paper, we propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed <em>entropy maximization</em> loss. Additionally, we introduce a <em>gradient penalty</em> loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over <strong>12%</strong> on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
	  <div class="container column is-four-fifths">
		<center>
			<h2 class="title is-3">Background/Foreground Bias in Action Recognition</h2>
		</center>
		<div id="results-carousel" class="carousel results-carousel">
			<div class="item active">
				<center>
					<h2 class="title is-4">Background Bias</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Original Video</h2>
							<img src="./static/videos/tennis_raw.gif" id="tennis-raw">
              <h3 class="title is-6">Baseline Prediction: <span style="color: green;">Tennis Swing</span></h3>
              <h3 class="title is-6">ALBAR Prediction: <span style="color: green;">Tennis Swing</span></h3>
						</div>
						<div>
							<h2 class="title is-5">Background Edit</h2>
							<img src="./static/videos/tennis_background.gif" id="tennis-background">
              <h3 class="title is-6">Baseline Prediction: <span style="color: red;">Horse Riding</span></h3>
              <h3 class="title is-6">ALBAR Prediction: <span style="color: green;">Tennis Swing</span></h3>
						</div>
					</div>
				</center>
			</div>
			<div class="item">
				<center>
					<h2 class="title is-4">Foreground Bias</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Single Video Frame</h2>
              <img src="./static/images/archery_frame.jpg" id="archery-static">
              <h3 class="title is-6">Baseline Prediction: <span style="color: red;">Passing American Football</span></h3>
              <h3 class="title is-6">ALBAR Prediction: <span style="color: red;"></span>Playing Accordion</h3>
						</div>
						<div>
              <h2 class="title is-5">Original Video</h2>
              <img src="./static/videos/archery_raw.gif" id="archery-raw">
              <h3 class="title is-6">Baseline Prediction: <span style="color: red;">Passing American Football</span></h3>
              <h3 class="title is-6">ALBAR Prediction: <span style="color: green;">Archery</span></h3>
						</div>
					</div>
				</center>
			</div>
			<!-- <div class="item">
				<center>
					<h2 class="title is-4">UCF-Crime: Shoplifting026_x264.mp4</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Raw</h2>
							<img src="./static/videos/raw_Shoplifting026_x264-Anomaly01.gif" id="shoplifting-raw">
						</div>
						<div>
							<h2 class="title is-5">Anonymized</h2>
							<img src="./static/videos/anon_Shoplifting026_x264-Anomaly01.gif" id="shoplifting-anon">
						</div>
					</div>
				</center>
			</div> -->
        </div>
    </div>
  </div>
</section>

<section class="section" id="paper-details">
	<div class="container is-max-desktop">
		<div class="columns is-centered">
			<h2 class="title is-3">Paper Details</h2>
		</div>
    <h2 class="title is-4">Method Diagram</h2>
    <div class="column has-text-justified">
      <div class="method-diagram">
	      <!--- <img style="padding-top:10px" src="./static/images/method_diagram.svg"> --->
        <img style="padding-top:10px" src="./static/images/main_architecture.svg">
        <h2 class="content has-text-justified">
          Full ALBAR method diagram showing two types clips passed through the same video encoder with different losses applied to each. 
          The first clip simply uses a standard cross entropy loss to learn to classify actions based on clips with motion. The second clip is created by sampling a frame from the first clip and repeating it to match the original clip shape, creating a static clip with no motion. The adversarial component is created by <em>subtracting</em> the cross entropy of the static clip prediction. This prediction is encouraged to be uncertain by the entropy loss, and the gradients w.r.t. the static prediction (shown in <span style="color:red;">red</span>) are encouraged to be lower for more stable training by the gradient penalty loss.
           <!-- A <em>negative gradient</em> cross entropy loss is applied to this clip, encouraging the model to predict <em>incorrect</em> action classes given a clip with no motion. To stabilize this adversarial training, additional <em>entropy maximization</em> and <em>gradient penalty</em> losses are applied to the static clip. The entropy maximization essentially causes the model to be highly uncertain about class predictions, while the gradient penalty prevents static inputs from having too large an effect on training. -->
          </h2>
      </div>
    </div>

    <h2 class="title is-4">Static Adversarial Loss</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        Many weakly supervised anomaly detection (WSAD) papers such as the <a href="https://arxiv.org/pdf/2211.15098.pdf">MGFN</a> model used in this work find that variation between feature magnitudes 
        of video segments are useful for localizing anomalous segments in videos. Based on this observation, we speculate that detecting anomalies in long, 
        untrimmed videos requires temporally distinctive reasoning to determine whether events in the same scene are anomalous. The figure below shows
        the separation objective of the magnitude contrastive loss proposed by the authors of <a href="https://arxiv.org/pdf/2211.15098.pdf">MGFN</a>.
      </h2>
      <div class="column is-centered">
        <img style="display: flex; width: 40%; height: 40%; margin: auto;" src="./static/images/mgfn_magcon.png">
      </div>
      
    </div>
  

    <h2 class="title is-4">Effect of Gradient Penalty</h2>
    <div class="column is-centered">
      <!-- <img style="display: flex; margin: auto; width: 35%; height: 35%;" src="./static/images/Temp-DistLoss.svg"> -->
      <!-- insert pdf -->
      <img style="display: flex; margin: auto; width: 35%; height: 35%;" src="./static/images/gradient_penalty.pdf">
    </div>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        Figure~\ref{fig:gradpen_effect} highlights the efficacy of our additional gradient penalty objective in stabilizing training and improving performance. It prevents the model from taking large steps in different directions as the static adversarial and temporal cross-entropy objectives fight back and forth, leading to overall smoother learning and improved overall performance.
        
      </h2>
    </div>
	</div>
</section>

<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Results</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        <p>
          Below are quantitative results highlighting the improvement when compared to existing debiasing techniques in both background and foreground debiasing performance across all protocols based on the HMDB51 dataset. ALBAR defines a strong SOTA by achieving impressive foreground debiasing performance, demonstrating the efficacy of the proposed static adversarial approach.</p>
      </h2>
      <img src="./static/images/results_table.png">
    </div>
  </div>
</section>

<section class="section" id="protocol_update">
  <div class="container is-max-desktop">
    <h2 class="title is-4">UCF101 Updated Protocol</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        <p>
          <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Mitigating_and_Evaluating_Static_Bias_of_Action_Representations_in_the_ICCV_2023_paper.html">StillMix</a> proposed SCUBA and SCUFO datasets and metrics to evaluate both background and foreground bias in video action recognition models. These protocols require masks to extract the foreground from the original clips. However, the masks used for the UCF101 variation are bounding boxes. Thus, surrounding background information is carried into the bias evaluation videos, as seen in Figure~\ref{fig:ucf_back}. This is not sufficient to evaluate performance on this dataset, as a classifier reliant on the background can still take advantage of this information to score high on the protocol. To mitigate this effect, we utilize a flexible video object segmentation model, <a href="https://arxiv.org/pdf/2305.06558">SAMTrack</a> to segment the actors (subjects) in each video. The actors are initially grounded using the same bounding boxes. Each testing video is manually checked for accurate segmentation. The StillMix dataset creation protocol is used to create SCUBA and SCUFO variations with these new masks. The improved benchmark no longer includes background information, such as in Figure~\ref{fig:ucf_back} (a), tightly bounding the human subject as seen in Figure~\ref{fig:ucf_back} (b). Results in Table~\ref{tab:ucf_scuba_small} show results on our newly created benchmark. See \supp{Appendix Sec.~\ref{sec:exp_details}} for results on the existing benchmark.
        </p>
      </h2>
      <img src="./static/images/results_table.png">
    </div>
  </div>
</section>

<section class="section" id="conclusion">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Conclusion</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        <p>
          We propose ALBAR, a novel label-free adversarial training framework for efficient background and foreground debiasing of video action recognition models. The framework eliminates the need for direct knowledge of biased attributes such as an additional critic model, instead leveraging the negative cross-entropy loss of a clip without motion passed through the same model as the adversarial component. To ensure optimal training, we incorporate static clip entropy maximization and gradient penalty objectives. We thoroughly validate the performance of our approach across a comprehensive suite of bias evaluation protocols, demonstrating its effectiveness and generalization across multiple datasets. Moreover, ALBAR can be seamlessly combined with existing debiasing augmentations to achieve performance that significantly surpasses the current state-of-the-art. It is our hope that our work contributes to the development of fair, unbiased, and trustworthy video understanding models.
        </p>
        <p>
          For more technical details and results, check out our attached main paper, thank you!
        </p>
      </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{fioresi2025albar,
  title={ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition},
  author={Fioresi, Joseph and Dave, Ishan Rajendrakumar and Shah, Mubarak},
  booktitle={Proceedings of the International Conference on Learning Representations},
  pages={13598--13609},
  year={2025}
}
</code></pre>
  </div>
</section>

<section class="section" id="acknowledgement">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Acknowledgement</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        This work was supported in part by the National Science Foundation (NSF) and Center for Smart Streetscapes (CS3) under NSF Cooperative Agreement No. EEC-2133516.
      </h2>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/images/albar_arxiv.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/joefioresi718" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the source code of the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>, which is
            licensed under a <a rel="license"
								href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
								Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
