<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ALBAR utilizes adversarial learning to mitigate foreground and background biases in video action recognition.">
  <meta name="keywords" content="action recognition, foreground bias, background bias, ALBAR, adversarial learning, ICLR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ucf_hotbar_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition [ICLR 2025]</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://joefioresi718.github.io/">Joseph Fioresi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://daveishan.github.io/">Ishan Rajendrakumar Dave</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en&oi=ao">Mubarak Shah</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Center for Research in Computer Vision (CRCV), University of Central Florida</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2308.11072"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/3a9qeJUD1GU?si=DlpYsj1AFW6iQX1J"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/UCF-CRCV/ALBAR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Poster Link -->
              <span class="link-block">
                <a href="./static/images/albar_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
	<center>
		<div class="container is-max-desktop">
			<div class="hero-body">
			  <img src="./static/images/TeaserFigure.svg" alt="ALBAR Teaser" class="teaser-image">
			  <h2 class="subtitle has-text-centered">
				TeD-SPAD removes private attributes from videos without requring any annotations.
				Anonymized videos can be used with minimal utility loss in video anomaly detection. 
			  </h2>
			</div>
		  </div>
	</center>
</section>

<style> 
    /* .results-carousel {
      width: 100%;
      height: 100vh;
      display: flex;
      flex-wrap: nowrap;
      scroll-snap-type: x mandatory;
      -webkit-overflow-scrolling: touch;
    } */

	.gif-pair {
      width: 100%;
      height: 100%;
      scroll-snap-align: start;
      display: flex;
      flex-wrap: wrap;
      justify-content: space-evenly;
      /* align-items: center;
	  align-self: center; */
      /* padding: 10px; */
    }

</style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases.
          </p>
          <p>
            In this paper, we propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed <em>entropy maximization</em> loss. Additionally, we introduce a <em>gradient penalty</em> loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over <strong>12%</strong> on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
	  <div class="container column is-four-fifths">
		<center>
			<h2 class="title is-3">Anonymization Visualizations</h2>
		</center>
		<div id="results-carousel" class="carousel results-carousel">
			<div class="item active">
				<center>
					<h2 class="title is-4">UCF-Crime: Assault033_x264.mp4</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Raw</h2>
							<img src="./static/videos/raw_Assault033_x264-Anomaly01.gif" id="assault-raw">
						</div>
						<div>
							<h2 class="title is-5">Anonymized</h2>
							<img src="./static/videos/anon_Assault033_x264-Anomaly01.gif" id="assault-anon">
						</div>
					</div>
				</center>
			</div>
			<div class="item">
				<center>
					<h2 class="title is-4">UCF-Crime: Fighting015_x264.mp4</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Raw</h2>
							<img src="./static/videos/raw_Fighting015_x264-Anomaly01.gif" id="fighting-raw">
						</div>
						<div>
							<h2 class="title is-5">Anonymized</h2>
							<img src="./static/videos/anon_Fighting015_x264-Anomaly01.gif" id="fighting-anon">
						</div>
					</div>
				</center>
			</div>
			<div class="item">
				<center>
					<h2 class="title is-4">UCF-Crime: Shoplifting026_x264.mp4</h2>
					<div class="gif-pair">
						<div>
							<h2 class="title is-5">Raw</h2>
							<img src="./static/videos/raw_Shoplifting026_x264-Anomaly01.gif" id="shoplifting-raw">
						</div>
						<div>
							<h2 class="title is-5">Anonymized</h2>
							<img src="./static/videos/anon_Shoplifting026_x264-Anomaly01.gif" id="shoplifting-anon">
						</div>
					</div>
				</center>
			</div>
        </div>
    </div>
  </div>
</section>

<section class="section" id="paper-details">
	<div class="container is-max-desktop">
		<div class="columns is-centered">
			<h2 class="title is-3">Paper Details</h2>
		</div>
    <h2 class="title is-4">Method Diagram</h2>
    <div class="column has-text-justified">
      <div class="method-diagram">
	      <!--- <img style="padding-top:10px" src="./static/images/method_diagram.svg"> --->
        <img style="padding-top:10px" src="./static/images/main_architecture.svg">
        <h2 class="content has-text-justified">
          Full ALBAR method diagram showing two types clips passed through the same video encoder with different losses applied to each. 
          The first clip simply uses a standard cross entropy loss to learn to classify actions based on clips with motion. The second clip is created by sampling a frame from the first clip and repeating it to match the original clip shape, creating a static clip with no motion. The adversarial component is created by <em>subtracting</em> the cross entropy of the static clip prediction. This prediction is encouraged to be uncertain by the entropy loss, and the gradients w.r.t. the static prediction (shown in <span style="color:red;">red</span>) are encouraged to be lower for more stable training by the gradient penalty loss.
           <!-- A <em>negative gradient</em> cross entropy loss is applied to this clip, encouraging the model to predict <em>incorrect</em> action classes given a clip with no motion. To stabilize this adversarial training, additional <em>entropy maximization</em> and <em>gradient penalty</em> losses are applied to the static clip. The entropy maximization essentially causes the model to be highly uncertain about class predictions, while the gradient penalty prevents static inputs from having too large an effect on training. -->
          </h2>
      </div>
    </div>

    <h2 class="title is-4">Anomaly Feature Representation Learning</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        Many weakly supervised anomaly detection (WSAD) papers such as the <a href="https://arxiv.org/pdf/2211.15098.pdf">MGFN</a> model used in this work find that variation between feature magnitudes 
        of video segments are useful for localizing anomalous segments in videos. Based on this observation, we speculate that detecting anomalies in long, 
        untrimmed videos requires temporally distinctive reasoning to determine whether events in the same scene are anomalous. The figure below shows
        the separation objective of the magnitude contrastive loss proposed by the authors of <a href="https://arxiv.org/pdf/2211.15098.pdf">MGFN</a>.
      </h2>
      <div class="column is-centered">
        <img style="display: flex; width: 40%; height: 40%; margin: auto;" src="./static/images/mgfn_magcon.png">
      </div>
      
    </div>
  

    <h2 class="title is-4">Self-Supervised Privacy Preservation</h2>
    <div class="column is-centered">
      <img style="display: flex; margin: auto; width: 35%; height: 35%;" src="./static/images/Temp-DistLoss.svg">
    </div>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        We propose a self-supervised privacy preservation method that uses a triplet loss to enforce temporal distinctiveness 
        of the learned feature representations. 

        <!-- We use a UNet architecture to anonymize each video frame along with I3D as the action classification/feature extraction model.  -->
        The anonymizer, utility feature extractor, and budget model are trained jointly in an adversarial manner. 
        The utility model is trained with a weighted combination of standard cross-entropy loss along with with the triplet contrastive loss,
        which is composed of a positive pair of stochastically augmented clips and a negative clip from a different timestep in the same video. 
        The learned temporally-distinctive representations are useful in the downstream anomaly detection task. In the next section, we show that
        even under anonymization constraints, these representations perform on par with raw video representations. 
        
        <!-- Need to update this section. -->
        <!-- We design a temporal-distinctive triplet loss, which increases the agreement 
        between temporally-aligned clips of the same video and increases dissimilarity between representations of the clips which are temporally misaligned. -->
        
      </h2>
    </div>
	</div>
</section>

<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Results</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        <p>
          Below are quantitative results highlighting the improvement when compared to existing debiasing techniques in both background and foreground debiasing performance across all protocols based on the HMDB51 dataset. ALBAR defines a strong SOTA by achieving impressive foreground debiasing performance, demonstrating the efficacy of the proposed static adversarial approach.</p>
      </h2>
      <img src="./static/images/results_table.png">
    </div>
  </div>
</section>

<section class="section" id="conclusion">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Conclusion</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        <p>
          We propose ALBAR, a novel label-free adversarial training framework for efficient background and foreground debiasing of video action recognition models. The framework eliminates the need for direct knowledge of biased attributes such as an additional critic model, instead leveraging the negative cross-entropy loss of a clip without motion passed through the same model as the adversarial component. To ensure optimal training, we incorporate static clip entropy maximization and gradient penalty objectives. We thoroughly validate the performance of our approach across a comprehensive suite of bias evaluation protocols, demonstrating its effectiveness and generalization across multiple datasets. Moreover, ALBAR can be seamlessly combined with existing debiasing augmentations to achieve performance that significantly surpasses the current state-of-the-art. It is our hope that our work contributes to the development of fair, unbiased, and trustworthy video understanding models.
        </p>
        <p>
          For more technical details and results, check out our attached main paper, thank you!
        </p>
      </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{fioresi2025albar,
  title={ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition},
  author={Fioresi, Joseph and Dave, Ishan Rajendrakumar and Shah, Mubarak},
  booktitle={Proceedings of the International Conference on Learning Representations},
  pages={13598--13609},
  year={2025}
}
</code></pre>
  </div>
</section>

<section class="section" id="acknowledgement">
  <div class="container is-max-desktop">
    <h2 class="title is-4">Acknowledgement</h2>
    <div class="column has-text-justified">
      <h2 class="content has-text-justified">
        This work was supported in part by the National Science Foundation (NSF) and Center for Smart Streetscapes (CS3) under NSF Cooperative Agreement No. EEC-2133516.
      </h2>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/images/albar_arxiv.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/joefioresi718" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the source code of the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>, which is
            licensed under a <a rel="license"
								href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
								Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
